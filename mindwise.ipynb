{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoewDtv/ft58pFZtKAaCj/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratayyy/MindWise/blob/main/mindwise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR3_07CmtwJo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import cv2\n",
        "import gym\n",
        "import random\n",
        "from gym import spaces\n",
        "from google.colab import widgets\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.offsetbox import OffsetImage, AnnotationBbox"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "\n",
        "MAX_TIMESTEPS = 300\n",
        "GAMMA = 0.99\n",
        "ALPHA = 0.3\n",
        "EPSILON = 1.0\n",
        "EPISODES = 1000\n",
        "\n",
        "class Environment(Enum):\n",
        "  Deterministic = 1\n",
        "  Stochastic = 2\n",
        "\n",
        "class Agent(Enum):\n",
        "  QLearning = 1  \n",
        "  DoubleQLearning = 2\n"
      ],
      "metadata": {
        "id": "FNGL1dN-1BUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "@dataclass\n",
        "class LearningParameter:\n",
        "  # Default values of learning parameters\n",
        "    epsilon: float = EPSILON \n",
        "    gamma: float = GAMMA\n",
        "    alpha: float = ALPHA\n",
        "    max_timesteps: int = MAX_TIMESTEPS\n",
        "    episodes: int = EPISODES\n",
        "    epsilon_decay:float = 0.99\n",
        "@dataclass\n",
        "class Experience:\n",
        "    state: int = None \n",
        "    action: int = None\n",
        "    next_state: int = None\n",
        "    reward: float = None   \n",
        "    done: bool = None   "
      ],
      "metadata": {
        "id": "PNIWdc3D3F6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GridEnvironment(gym.Env):\n",
        "    metadata = { 'render.modes': [] }\n",
        "    ROWS = 9\n",
        "    COLS = 9\n",
        "\n",
        "\n",
        "    NO_OF_ACTIONS = 8\n",
        "\n",
        "    def remove_points(self,arr:np.ndarray, points:list):\n",
        "          arr_list = list(arr)\n",
        "          for i in range(len(points)):\n",
        "             for j in range(len(arr)):\n",
        "               if(all(list(arr[j]) == points[i])):\n",
        "                 arr_list.pop(j)\n",
        "          return np.asarray(arr_list)\n",
        "  \n",
        "\n",
        "    def set_artifact_positions(self,environment:Environment):\n",
        "        self.agent_pos = np.asarray([0, 0])\n",
        "        self.goal_pos = np.asarray([8, 8])\n",
        "        self.demon_pos = np.asarray([4, 4])\n",
        "        stochastic_toxin_pos = [[[3,2], [7,6], [2,7]],[[4,3],[8,5],[3,6]]] \n",
        "        stochastic_hp_pos = [[[4,2], [8,6], [3,7]],[[3,3],[8,4],[3,5]]]\n",
        "\n",
        "\n",
        "        if(environment == Environment.Deterministic ):\n",
        "          self.toxin_pos = np.asarray([[3,2], [7,6], [2,7]])\n",
        "          self.hp_pos = np.asarray([[1,1], [3,4], [4,7]])\n",
        "        else:\n",
        "          self.toxin_pos = np.asarray(random.choices(stochastic_toxin_pos, weights=(10, 90), k=1)[0])  \n",
        "          self.hp_pos = np.asarray(random.choices(stochastic_hp_pos, weights=(10, 90), k=1)[0])\n",
        "       \n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self, environment_type=Environment.Deterministic, render_trace = False, parameter=LearningParameter()):\n",
        "        self.environment_type = environment_type\n",
        "        self.observation_space = spaces.Discrete(self.ROWS * self.COLS)\n",
        "        self.action_space = spaces.Discrete(self.NO_OF_ACTIONS)\n",
        "        self.max_timesteps = MAX_TIMESTEPS\n",
        "        self.timesteps = 0\n",
        "        self.render_trace = render_trace\n",
        "\n",
        "        # Create a map of coordinates and state\n",
        "        self.coordinates_state_mapping = {}\n",
        "        for i in range(self.ROWS):\n",
        "            for j in range(self.COLS):\n",
        "                self.coordinates_state_mapping[f'{np.asarray([j, i])}'] = i * self.COLS + j\n",
        "                \n",
        "    def reset(self):\n",
        "        self.timestep = 0\n",
        "        self.set_artifact_positions(self.environment_type)\n",
        "\n",
        "        # return the agent_pos coordinates\n",
        "        observation = self.coordinates_state_mapping[f'{self.agent_pos}']\n",
        "        return observation\n",
        "\n",
        "    def step(self, action):\n",
        "      \n",
        "        #Actions\n",
        "        if action == 0:\n",
        "          self.agent_pos[0] += 1 # Agent moves right\n",
        "        elif action == 1:\n",
        "          self.agent_pos[0] -= 1 # Agent moves left\n",
        "        elif action == 2:\n",
        "          self.agent_pos[1] += 1 # Agent moves up\n",
        "        elif action == 3:\n",
        "          self.agent_pos[1] -= 1 # Agent moves down\n",
        "        elif action == 4:        # Agent moves down_right\n",
        "          self.agent_pos[0] += 1 \n",
        "          self.agent_pos[1] += 1\n",
        "        elif action == 5:        # Agent moves top_left\n",
        "          self.agent_pos[0] -= 1\n",
        "          self.agent_pos[1] -= 1\n",
        "        elif action == 6:        # Agent moves top_right\n",
        "          self.agent_pos[0] += 1\n",
        "          self.agent_pos[1] -= 1\n",
        "        elif action == 7:        # Agent moves down_left\n",
        "          self.agent_pos[0] -= 1  \n",
        "          self.agent_pos[1] += 1  \n",
        "            \n",
        "\n",
        "        # Clipping\n",
        "        self.agent_pos = np.clip(self.agent_pos, a_min=[0, 0],\n",
        "                                 a_max=[self.ROWS - 1, self.COLS - 1])\n",
        "      \n",
        "        observation = self.coordinates_state_mapping[f'{np.asarray(self.agent_pos)}']\n",
        "           \n",
        "\n",
        "        #Reward assignment\n",
        "        reward = -1\n",
        "        toxin_reward = -200\n",
        "        hp_reward = 200\n",
        "        goal_reward = 2000\n",
        "        demon_reward = -2000\n",
        "        \n",
        "              \n",
        "\n",
        "        done = False\n",
        "\n",
        "        # Render is called here to ensure tile-agent interaction visualisation is preserved\n",
        "        # (Artifacts once popped out of the array will remove the \"interaction\" for visualisation logic to use)\n",
        "        if self.render_trace:\n",
        "          with output_grid.output_to(0, 0):\n",
        "            output_grid.clear_cell()\n",
        "            self.render()\n",
        "\n",
        "       \n",
        "       # Terminate if goal is attained\n",
        "        if (self.agent_pos == self.goal_pos).all():\n",
        "            reward += goal_reward\n",
        "            done = True\n",
        "\n",
        "       # Terminate if interacts with demon\n",
        "        elif(self.agent_pos == self.demon_pos).all():\n",
        "            reward += demon_reward \n",
        "            done = True   \n",
        "      \n",
        "        # if hp_pos is encountered, collect reward and pop the hp from hp_pos\n",
        "        elif any(((hp_pos == self.agent_pos).all() for hp_pos in self.hp_pos)):      \n",
        "          for i in range(len(self.hp_pos)):\n",
        "              if (self.agent_pos == self.hp_pos[i]).all():\n",
        "                reward += hp_reward\n",
        "                self.hp_pos = self.remove_points(self.hp_pos,[self.hp_pos[i]]) \n",
        "                break \n",
        "                \n",
        "\n",
        "        # if toxin_pos is encountered, collect reward and pop the hp from toxin_pos\n",
        "        elif any(((toxin_pos == self.agent_pos).all() for toxin_pos in self.toxin_pos)):          \n",
        "          for i in range(len(self.toxin_pos)):\n",
        "              if (self.agent_pos == self.toxin_pos[i]).all():\n",
        "                reward += toxin_reward\n",
        "                self.toxin_pos = self.remove_points(self.toxin_pos,[self.toxin_pos[i]])      \n",
        "                break\n",
        "\n",
        "        # Ensure agent is motivated to optimize moves\n",
        "        else:\n",
        "          reward-=1        \n",
        "                  \n",
        "\n",
        "        self.timestep += 1\n",
        "\n",
        "        if (self.timestep >= self.max_timesteps):\n",
        "          done = True\n",
        "\n",
        "        info = {}\n",
        "       \n",
        "\n",
        "        return observation, reward, done, info\n",
        "\n",
        "    def render(self, mode='human', plot=False):\n",
        "        \"\"\"This method renders the environment.\n",
        "\n",
        "        :param str mode: 'human' renders to the current display or terminal and returns nothing.\n",
        "        :param bool plot: Boolean indicating whether we show a plot or not. If False, the method returns a resized NumPy\n",
        "                     array representation of the environment to be used as the state. If True it plots the environment.\n",
        "\n",
        "        :returns arr preprocessed_image: Grayscale NumPy array representation of the environment.\"\"\"\n",
        "\n",
        "        fig, ax = plt.subplots(figsize=(8, 8))\n",
        "        ax.set_xlim(0, 9)\n",
        "        ax.set_ylim(0, 9)\n",
        "\n",
        "        if(self.environment_type == Environment.Deterministic):\n",
        "          ax.set_title('Deterministic Learning Env')\n",
        "        else:\n",
        "          ax.set_title('Stochastic Learning Env') \n",
        "\n",
        "        def plot_image(plot_pos):\n",
        "            \"\"\"This is a helper function to render the environment. It checks which objects are in a particular\n",
        "            position on the grid and renders the appropriate image.\n",
        "\n",
        "            :param arr plot_pos: Co-ordinates of the grid position which needs to be rendered.\"\"\"\n",
        "\n",
        "            # Initially setting every object to not be plotted.\n",
        "            plot_agent, plot_hp, plot_goal, plot_toxin, plot_demon = \\\n",
        "                False, False, False, False, False\n",
        "\n",
        "            # Checking which objects need to be plotted by comparing their positions.\n",
        "            if np.array_equal(self.agent_pos, plot_pos):\n",
        "                plot_agent = True\n",
        "            if any(np.array_equal(self.hp_pos[i], plot_pos) for i in range(len(self.hp_pos))):\n",
        "                plot_hp = True\n",
        "        \n",
        "            if any(np.array_equal(self.toxin_pos[i], plot_pos) for i in range(len(self.toxin_pos))):\n",
        "                plot_toxin = True\n",
        "       \n",
        "            if np.array_equal(plot_pos, self.demon_pos):\n",
        "                plot_demon = True\n",
        "\n",
        "            if np.array_equal(plot_pos, self.goal_pos):\n",
        "                plot_goal = True    \n",
        "\n",
        "            # Plot for Agent.\n",
        "            if plot_agent and \\\n",
        "                    all(not item for item in\n",
        "                        [plot_hp, plot_goal, plot_toxin, plot_demon]):\n",
        "                agent = AnnotationBbox(OffsetImage(plt.imread('images/agent.png'), zoom=0.3),\n",
        "                                       np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(agent)\n",
        "\n",
        "          \n",
        "            # Plot for Toxin.\n",
        "            elif plot_toxin and \\\n",
        "                    all(not item for item in\n",
        "                        [plot_agent, plot_hp, plot_goal, plot_demon]):\n",
        "                toxin = AnnotationBbox(OffsetImage(plt.imread('images/toxin.png'), zoom=0.3),\n",
        "                                     np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(toxin)\n",
        "           \n",
        "            # Plot for HP.\n",
        "            elif plot_hp and \\\n",
        "                    all(not item for item in\n",
        "                        [plot_agent, plot_toxin, plot_goal, plot_demon]):\n",
        "                hp = AnnotationBbox(OffsetImage(plt.imread('images/hp.png'), zoom=0.3),\n",
        "                                     np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(hp)     \n",
        "\n",
        "            # Plot for Demon.\n",
        "            elif plot_demon and \\\n",
        "                    all(not item for item in\n",
        "                        [plot_agent, plot_hp, plot_goal, plot_toxin]):\n",
        "                demon = AnnotationBbox(OffsetImage(plt.imread('images/demon.png'), zoom=0.3),\n",
        "                                        np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(demon)\n",
        "\n",
        "            # Plot for Goal.\n",
        "            elif plot_goal and \\\n",
        "                    all(not item for item in\n",
        "                        [plot_agent, plot_hp, plot_demon, plot_toxin]):\n",
        "                goal = AnnotationBbox(OffsetImage(plt.imread('images/goal.png'), zoom=0.3),\n",
        "                                        np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(goal)    \n",
        "\n",
        "            # Plot for Agent and HP.\n",
        "            elif all( [plot_agent, plot_hp]) and \\\n",
        "                    not(all(\n",
        "                        [plot_goal, plot_toxin, plot_demon])):\n",
        "                agent_hp = AnnotationBbox(OffsetImage(plt.imread('images/agent_hp.png'), zoom=0.3),\n",
        "                                              np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(agent_hp)\n",
        "\n",
        "            # Plot for Agent and Toxin.\n",
        "            elif all(item for item in [plot_agent, plot_toxin]) and \\\n",
        "                    all(not item for item in\n",
        "                        [plot_hp, plot_goal, plot_demon]):\n",
        "                agent_toxin = AnnotationBbox(OffsetImage(plt.imread('images/agent_toxin.png'), zoom=0.3),\n",
        "                                           np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(agent_toxin)\n",
        "\n",
        "\n",
        "            # Plot for Agent and Demon.\n",
        "            elif all(item for item in [plot_agent, plot_demon]) and \\\n",
        "                    all(not item for item in\n",
        "                        [plot_goal, plot_toxin, plot_hp]):\n",
        "                agent_demon = AnnotationBbox(OffsetImage(plt.imread('images/agent_demon.png'),\n",
        "                                                          zoom=0.3), np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(agent_demon)\n",
        "\n",
        "            # Plot for Agent and Goal.\n",
        "            elif all(item for item in [plot_agent, plot_goal]) and \\\n",
        "                    all(not item for item in\n",
        "                        [plot_demon, plot_toxin, plot_hp]):\n",
        "                agent_goal = AnnotationBbox(OffsetImage(plt.imread('images/agent_goal.png'),\n",
        "                                                          zoom=0.3), np.add(plot_pos, [0.5, 0.5]), frameon=False)\n",
        "                ax.add_artist(agent_goal)    \n",
        "\n",
        "         \n",
        "        # Create a map of state and coordinates \n",
        "        state_coordinates_mapping = {}\n",
        "        for j in range(self.ROWS * self.COLS):\n",
        "            state_coordinates_mapping[j] = np.asarray(\n",
        "                [j % self.COLS, int(np.floor(j / self.COLS))])\n",
        "\n",
        "        # Rendering the images for all states.\n",
        "        for position in state_coordinates_mapping:\n",
        "            plot_image(state_coordinates_mapping[position])\n",
        "\n",
        "        plt.xticks([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
        "        plt.yticks([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
        "        plt.grid()  # Setting the plot to be of the type 'grid'.\n",
        "\n",
        "        if plot:  # Displaying the plot.\n",
        "            plt.show()\n",
        "        else:  # Returning the preprocessed image representation of the environment.\n",
        "            fig.canvas.draw()\n",
        "            img = np.array(fig.canvas.renderer.buffer_rgba())[:, :, :3]\n",
        "            width = 84 \n",
        "            height = 84\n",
        "            dim = (width, height)\n",
        "            # noinspection PyUnresolvedReferences\n",
        "            preprocessed_image = cv2.resize(img, dim, interpolation=cv2.INTER_AREA)\n",
        "            plt.show()\n",
        "            return preprocessed_image"
      ],
      "metadata": {
        "id": "M1R4HEXK3gLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomAgent:\n",
        "  def __init__(self, environment, agent=Agent.QLearning, parameter=LearningParameter()):\n",
        "    \n",
        "    self.env = environment\n",
        "    self.observation_space = environment.observation_space\n",
        "    self.action_space = environment.action_space\n",
        "\n",
        "    self.alpha = parameter.alpha\n",
        "    self.gamma = parameter.gamma\n",
        "    self.epsilon = parameter.epsilon\n",
        "    self.max_timesteps = parameter.max_timesteps\n",
        "    self.episodes = parameter.episodes\n",
        "    self.epsilon_decay = parameter.epsilon_decay\n",
        "    self.agent = agent\n",
        "    if agent == Agent.QLearning:\n",
        "      self.q_table= np.zeros((self.observation_space.n, self.action_space.n))\n",
        "    else:\n",
        "      self.q_table_1= np.zeros((self.observation_space.n, self.action_space.n))\n",
        "      self.q_table_2= np.zeros((self.observation_space.n, self.action_space.n))\n",
        "\n",
        "\n",
        "\n",
        "  def q_step(self, state):\n",
        "    q_state= self.q_table[state]\n",
        "    exploit= np.argmax(q_state)\n",
        "    explore= np.random.choice(self.action_space.n)\n",
        "\n",
        "    # Depending on the epsilon value greedy or random action is chosen\n",
        "    return explore if random.random() < self.epsilon else exploit\n",
        "\n",
        "  def double_q_step(self, state):\n",
        "    q_state_1 = self.q_table_1[state]  \n",
        "    q_state_2 = self.q_table_2[state]\n",
        "    exploit = np.argmax(q_state_1 + q_state_2 ) # greedy policy \n",
        "    explore= np.random.choice(self.action_space.n)\n",
        "\n",
        "    return explore if random.random() < self.epsilon else exploit\n",
        "     \n",
        "\n",
        "  def use_q_learning(self, experience):\n",
        "    state, action, next_state, reward, done= experience\n",
        "    \n",
        "    # Q-value calculation\n",
        "    q_next_value= np.zeros([self.action_space.n]) if done else self.q_table[next_state]\n",
        "    q_value= reward + self.gamma * np.max(q_next_value) - self.q_table[state, action]\n",
        "    self.q_table[state, action] += self.alpha * q_value\n",
        "\n",
        "\n",
        "    if done:\n",
        "      # Setting exponential epsilon decay to prefer exploitation over exploration\n",
        "      self.epsilon= self.epsilon * self.epsilon_decay\n",
        "\n",
        "\n",
        "  def use_double_q_learning(self, experience):\n",
        "    state, action, next_state, reward, done= experience\n",
        "\n",
        "    useQa = True if random.random() > 0.5 else False # Randomly use either of the tables\n",
        "\n",
        "    # Q value computation for table 1\n",
        "    # Qa(s,a) = Qa(s,a) + alpha *[ reward + gamma(Qb(st+1,argmax(Qa(st+1,a`))))]\n",
        "    q_table_1_next_actions = np.zeros([self.action_space.n]) if done else self.q_table_1[next_state]\n",
        "    argmax_q_table_1 = np.argmax(q_table_1_next_actions)\n",
        "    q_target = self.gamma * self.q_table_2[next_state,argmax_q_table_1]\n",
        "    q_value_1 = reward + self.gamma * q_target - self.q_table_1[state, action]\n",
        "    self.q_table_1[state, action] += self.alpha * q_value_1\n",
        "\n",
        "\n",
        "    # Q value computation for table 2\n",
        "    # Qb(s,a) = Qb(s,a) + alpha *[ reward + gamma(Qa(st+1,argmax(Qb(st+1,a`))))]\n",
        "    q_table_2_next_actions = np.zeros([self.action_space.n]) if done else self.q_table_2[next_state]\n",
        "    argmax_q_table_2 = np.argmax(q_table_2_next_actions)\n",
        "    q_target = self.gamma * self.q_table_2[next_state,argmax_q_table_2]\n",
        "    q_value_2 = reward + self.gamma * q_target - self.q_table_2[state, action]\n",
        "    self.q_table_2[state, action] += self.alpha * q_value_2\n",
        "\n",
        "    if done:\n",
        "      # Setting exponential epsilon decay to prefer exploitation over exploration\n",
        "      self.epsilon= self.epsilon * self.epsilon_decay\n",
        "\n",
        "\n",
        "\n",
        "  def train(self, experience:Experience):\n",
        "    if self.agent == Agent.QLearning :\n",
        "      return self.use_q_learning(experience)\n",
        "    else:\n",
        "      return self.use_double_q_learning(experience)  \n",
        "      \n",
        "\n",
        "\n",
        "  def step(self, observation):\n",
        "    if self.agent == Agent.QLearning:\n",
        "      return self.q_step(observation)\n",
        "    else:\n",
        "      return self.double_q_step(observation)\n",
        "    "
      ],
      "metadata": {
        "id": "8FeaHdrY4RmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "class Simulator:\n",
        "  def __init__(self, environment:GridEnvironment, agent:Agent = None,parameter=LearningParameter() ):\n",
        "    \n",
        "    self.env = environment\n",
        "    self.agent_type = agent\n",
        "    self.agent = RandomAgent(environment,agent= Agent.QLearning if self.agent_type == None else self.agent_type,parameter=parameter )\n",
        "\n",
        "  \n",
        "  \n",
        "  def simulate(self, render_graph = False):\n",
        "    penalty_state  = []\n",
        "    episode_timestep = []\n",
        "    epsilon_decay = []\n",
        "    mean_reward = [] \n",
        "    episode_reward = []\n",
        "    success = []\n",
        "    succeeded = 0\n",
        "    failed = 0\n",
        "    cumulative_reward= 0\n",
        "\n",
        "  \n",
        "\n",
        "    for episode in range(self.agent.episodes):\n",
        "      state= self.env.reset()\n",
        "      done= False\n",
        "      reward_per_episode = timestep_per_episode = penalty_states_reached = 0\n",
        "    \n",
        "\n",
        "      while not done:\n",
        "        action= self.agent.step(observation= state)\n",
        "        next_state, reward, done, info= self.env.step(action= action)\n",
        "        self.agent.train(experience= (state, action, next_state, reward, done))\n",
        "        state= next_state\n",
        "\n",
        "        reward_per_episode += reward\n",
        "        cumulative_reward += reward\n",
        "        timestep_per_episode += 1\n",
        "        if reward < 0:\n",
        "          penalty_states_reached+=1\n",
        "\n",
        "      penalty_state.append(penalty_states_reached)\n",
        "      episode_timestep.append(timestep_per_episode)\n",
        "      epsilon_decay.append(self.agent.epsilon)\n",
        "      episode_reward.append(reward_per_episode)\n",
        "      mean_reward.append(reward_per_episode/self.agent.episodes)\n",
        "      \n",
        "      if done == True and self.env.timestep < self.agent.max_timesteps and reward_per_episode: # Positive reward\n",
        "        # Agent has reached the goal\n",
        "        succeeded += 1\n",
        "        success.append(1)\n",
        "      elif done == True and self.env.timestep == self.agent.max_timesteps:\n",
        "        # Agent ran out of time & has not reached the goal\n",
        "        failed += 1\n",
        "        success.append(0)\n",
        "\n",
        "    \n",
        "    if render_graph:\n",
        "        grid= widgets.Grid(3,2)\n",
        "        print('Performance of {} agent in \\n{} Environment'\\\n",
        "          .format(\"QLearning\" if self.agent_type == Agent.QLearning else \"DoubleQLearning\",\\\n",
        "                  \"Deterministic\" if self.env.environment_type == Environment.Deterministic else \"Stochastic\"))\n",
        "\n",
        "    # Plot Cumulative Rewards vs Episodes \n",
        "        with grid.output_to(0, 0):\n",
        "          plt.title('Cumulative reward per episode')\n",
        "          plt.plot(episode_reward)\n",
        "          plt.xlabel('Episodes')\n",
        "          plt.ylabel('Reward')  \n",
        "\n",
        "    # Plot Average Rewards vs Episodes \n",
        "        with grid.output_to(0, 1):\n",
        "          plt.title('Average reward per episode')\n",
        "          plt.plot(mean_reward)\n",
        "          plt.xlabel('Episodes')\n",
        "          plt.ylabel('Reward') \n",
        "\n",
        "    # Plot Epsilon Decay vs Episodes \n",
        "        with grid.output_to(1, 0):\n",
        "          plt.title('Epsilon decay')\n",
        "          plt.plot(epsilon_decay)\n",
        "          plt.xlabel('Episode')\n",
        "          plt.ylabel('Epsilon')\n",
        "\n",
        "    # Plot Episode vs AverageTimesteps\n",
        "        with grid.output_to(1, 1):\n",
        "          plt.title('Average Timesteps per episode')\n",
        "          plt.plot(episode_timestep)\n",
        "          plt.xlabel('Episode')\n",
        "          plt.ylabel('Timesteps')  \n",
        "\n",
        "    # Plot Episode vs AverageBadmoves\n",
        "        with grid.output_to(2, 0):\n",
        "         plt.title('Average bad moves per episode')\n",
        "         plt.plot(penalty_state)\n",
        "         plt.xlabel('Episode')\n",
        "         plt.ylabel('Badmoves')     \n",
        "\n",
        "        with grid.output_to(2, 1):\n",
        "          y_axis= [np.count_nonzero(i) for i in list(np.array(success).reshape(10,100))]\n",
        "          fig= plt.figure()\n",
        "          axis= fig.add_axes([0, 0, 1, 1])\n",
        "          axis.bar([i for i in range(1, 11)], y_axis, width= 0.25)\n",
        "          axis.set_title('Success rate by \\nagent for every {} episodes'.format(100))\n",
        "          axis.set_xlabel('Episodes (*100)')\n",
        "          axis.set_ylabel('Success rate')\n",
        "          fig.show()\n",
        "\n",
        "    Metrics = namedtuple ('Metrics',['succeeded','failed','cumulative_reward','penalty_state','episode_timestep','episode_reward','epsilon_decay','mean_reward'])\n",
        "    return Metrics(succeeded,failed,cumulative_reward,penalty_state,episode_timestep,episode_reward,epsilon_decay,mean_reward)\n",
        "    "
      ],
      "metadata": {
        "id": "nubBJ_TQ4hRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Random Agent exploring without learning\n",
        "env = GridEnvironment(render_trace=True)\n",
        "agent = RandomAgent(env)\n",
        "\n",
        "obs = env.reset()\n",
        "done = False\n",
        "\n",
        "output_grid = widgets.Grid(1,1)\n",
        "with output_grid.output_to(0, 0):\n",
        "  env.render()\n",
        "\n",
        "while not done:\n",
        "  action = agent.step(obs)\n",
        "  obs, reward, done, info = env.step(action)\n",
        "  time.sleep(0.5)"
      ],
      "metadata": {
        "id": "TG3S8QAa47uL",
        "outputId": "14e7d6bc-6255-4bd1-b422-a686618aa2fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<style>\n",
              "       table#id2, #id2 > tbody > tr > th, #id2 > tbody > tr > td {\n",
              "         border: 1px solid lightgray;\n",
              "         border-collapse:collapse;\n",
              "         \n",
              "        }</style>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table id=id2><tr><td id=id2-0-0></td></tr></table>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "window[\"a68ef73c-e91d-11ed-998d-0242ac1c000c\"] = google.colab.output.getActiveOutputArea();\n",
              "//# sourceURL=js_1ca83fc5f6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "window[\"a68fb8c0-e91d-11ed-998d-0242ac1c000c\"] = document.querySelector(\"#id2-0-0\");\n",
              "//# sourceURL=js_d99a4a421e"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "window[\"a6906c84-e91d-11ed-998d-0242ac1c000c\"] = google.colab.output.setActiveOutputArea(window[\"a68fb8c0-e91d-11ed-998d-0242ac1c000c\"]);\n",
              "//# sourceURL=js_855fc6c6c6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "window[\"a69b432a-e91d-11ed-998d-0242ac1c000c\"] = google.colab.output.setActiveOutputArea(window[\"a68ef73c-e91d-11ed-998d-0242ac1c000c\"]);\n",
              "//# sourceURL=js_181770d448"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ebeda05d2079>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0moutput_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwidgets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0moutput_grid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m             )\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mrender_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-d2bc24e604a5>\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, plot)\u001b[0m\n\u001b[1;32m    275\u001b[0m         \u001b[0;31m# Rendering the images for all states.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate_coordinates_mapping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mplot_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_coordinates_mapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxticks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-d2bc24e604a5>\u001b[0m in \u001b[0;36mplot_image\u001b[0;34m(plot_pos)\u001b[0m\n\u001b[1;32m    196\u001b[0m                     all(not item for item in\n\u001b[1;32m    197\u001b[0m                         [plot_hp, plot_goal, plot_toxin, plot_demon]):\n\u001b[0;32m--> 198\u001b[0;31m                 agent = AnnotationBbox(OffsetImage(plt.imread('images/agent.png'), zoom=0.3),\n\u001b[0m\u001b[1;32m    199\u001b[0m                                        np.add(plot_pos, [0.5, 0.5]), frameon=False)\n\u001b[1;32m    200\u001b[0m                 \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_artist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   2193\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2194\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2195\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, format)\u001b[0m\n\u001b[1;32m   1561\u001b[0m             \u001b[0;34m\"``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m             )\n\u001b[0;32m-> 1563\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mimg_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1564\u001b[0m         return (_pil_png_to_float_array(image)\n\u001b[1;32m   1565\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImagePlugin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPngImageFile\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/ImageFile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, fp, filename)\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;31m# filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'images/agent.png'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}